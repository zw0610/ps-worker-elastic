diff --git a/tensorflow/python/distribute/coordinator/cluster_coordinator.py b/tensorflow/python/distribute/coordinator/cluster_coordinator.py
index d7887336cbe..01404c2e93e 100644
--- a/tensorflow/python/distribute/coordinator/cluster_coordinator.py
+++ b/tensorflow/python/distribute/coordinator/cluster_coordinator.py
@@ -40,6 +40,7 @@ from tensorflow.python.eager import context
 from tensorflow.python.eager import def_function
 from tensorflow.python.eager import executor
 from tensorflow.python.eager import function as tf_function
+from tensorflow.python.eager import remote
 from tensorflow.python.framework import errors
 from tensorflow.python.framework import func_graph
 from tensorflow.python.framework import ops
@@ -288,7 +289,7 @@ def _maybe_as_type_spec(val):
 
 @tf_export("distribute.experimental.coordinator.PerWorkerValues", v1=[])
 class PerWorkerValues(object):
-  """A container that holds a list of values, one value per worker.
+  """A container that holds a dict of values, one value per worker.
 
   `tf.distribute.experimental.coordinator.PerWorkerValues` contains a collection
   of values, where each of the values is located on its corresponding worker,
@@ -305,7 +306,12 @@ class PerWorkerValues(object):
   """
 
   def __init__(self, values):
-    self._values = tuple(values)
+    if isinstance(values, list):
+      self._values = {i: values[i] for i in range(len(values))}
+    elif isinstance(values, dict):
+      self._values = {key: value for key, value in values.items()}
+    else:
+      raise ValueError("values must be of type list or dict.")
 
 
 def _select_worker_slice(worker_id, structured):
@@ -770,13 +776,29 @@ class Worker(object):
     self._should_worker_thread_run = True
 
     # Worker threads need to start after `Worker`'s initialization.
-    threading.Thread(target=self._process_queue,
+    self._process_thread = threading.Thread(target=self._process_queue,
                      name="WorkerClosureProcessingLoop-%d" % self.worker_index,
-                     daemon=True).start()
+                     daemon=True)
+    self._process_thread.start()
 
   def stop(self):
     """Ensure the worker thread is closed."""
     self._should_worker_thread_run = False
+    self._set_resources_aborted()
+
+  def restart(self):
+    """Restart the stopped worker."""
+    if self._should_worker_thread_run:
+      return
+    # Join the thread here the prevent deadlock in _closure_queue.get()
+    if self._process_thread is not None:
+      self._process_thread.join()
+    self._should_worker_thread_run = True
+    # Worker threads need to start after `Worker`'s initialization.
+    self._process_thread = threading.Thread(target=self._process_queue,
+                     name="WorkerClosureProcessingLoop-%d" % self.worker_index,
+                     daemon=True)
+    self._process_thread.start()
 
   def _set_resources_aborted(self):
     # TODO(yuefengz): maybe we can query whether a tensor is valid or not
@@ -833,6 +855,7 @@ class Worker(object):
     while self._should_worker_thread_run:
       closure = self._cluster._closure_queue.get()  # pylint: disable=protected-access
       if not self._should_worker_thread_run or closure is None:
+        self._cluster._closure_queue.put_back(closure)
         return
       self._process_closure(closure)
       # To properly stop the worker and preemption threads, it is important that
@@ -1071,6 +1094,8 @@ class ClusterCoordinator(object):
     self._strategy = strategy
     self.strategy.extended._used_with_coordinator = True
     self._cluster = Cluster(strategy)
+    self._cluster_spec = self._strategy._cluster_resolver.cluster_spec()
+    self._stopped_workers = {}
 
   def __del__(self):
     self._cluster.stop()
@@ -1275,10 +1300,10 @@ class ClusterCoordinator(object):
       wraps a tuple of `tf.distribute.experimental.coordinator.RemoteValue`
       objects.
     """
-    results = []
+    results = {}
     for w in self._cluster.workers:
-      results.append(w._create_resource(fn, args=args, kwargs=kwargs))  # pylint: disable=protected-access
-    return PerWorkerValues(tuple(results))
+      results[w.worker_index] = w._create_resource(fn, args=args, kwargs=kwargs)  # pylint: disable=protected-access
+    return PerWorkerValues(results)
 
   def fetch(self, val):
     """Blocking call to fetch results from the remote values.
@@ -1338,6 +1363,78 @@ class ClusterCoordinator(object):
     # TODO(yuefengz): we should fetch values in a batch.
     return nest.map_structure(_maybe_fetch, val)
 
+  def remove_worker(self, address):
+    """Remove a worker to the cluster.
+
+    Args:
+      address: The address of the worker to remove.
+    """
+    task_indices = self._cluster_spec.task_indices("worker")
+    sorted(task_indices)
+    for i in range(len(task_indices)):
+      task_index = task_indices[i]
+      job_address = self._cluster_spec.task_address("worker", task_index)
+      if job_address == address:
+        self._cluster.workers[i].stop()
+        self._stopped_workers[address] = task_index
+        return
+    logging.warning(
+        "Worker address %r not exists in current cluster." % address)
+
+  def add_worker(self, address):
+    """Add a worker to the cluster.
+
+    Args:
+      address: The address of the worker to add.
+    """
+    if address in self._stopped_workers:
+      # If the address is a stopped worker, restart it.
+      remote.connect_to_cluster(
+          self._cluster_spec,
+          job_name="chief")
+      index = self._stopped_workers[address]
+      del self._stopped_workers[address]
+      self._cluster.workers[index].restart()
+      return
+
+    # Check if the address already exists in current cluster.
+    for job_name in parameter_server_strategy_v2.ALLOWED_TASK_TYPES:
+      for task_address in self._cluster_spec.job_tasks(job_name):
+        if task_address == address:
+          if job_name == "worker":
+            logging.warning(
+                "Worker address %r already active in current cluster." % address)
+            return
+          else:
+            raise ValueError(
+                "New worker address %r should not be the same as %r." %
+                (address, job_name))
+
+    # Start create the new cluster spec.
+    if self._stopped_workers:
+      # Removed the stopped workers from the cluster.
+      for i in range(len(self._cluster.workers)):
+        for _, stopped_index in self._stopped_workers.items():
+          if self._cluster.workers[i].worker_index == stopped_index:
+            self._cluster_spec.remove_task("worker", stopped_index)
+            self._cluster.workers[i] = None
+      self._stopped_workers = {}
+      self._cluster.workers = [w for w in self._cluster.workers
+                               if w is not None]
+    # Decide the task index for the new worker
+    index = 0
+    if self._cluster.workers:
+      index = self._cluster.workers[-1].worker_index + 1
+    self._cluster_spec.add_task("worker", index, address)
+    remote.connect_to_cluster(
+        self._cluster_spec,
+        job_name="chief")
+    # Update the ServerDef in the failure handler
+    self._cluster.failure_handler._server_def = context.get_server_def()
+    self._cluster.workers.append(
+        Worker(index, "/job:worker/replica:0/task:%d" % index, self._cluster)
+    )
+
 
 class _PerWorkerDistributedDataset(object):
   """Represents worker-distributed datasets created from dataset function."""
@@ -1385,7 +1482,7 @@ class _PerWorkerDistributedDataset(object):
 
     # Setting type_spec of each RemoteValue so that functions taking these
     # RemoteValues as inputs can be traced.
-    for iterator_remote_value in per_worker_iterator._values:
+    for iterator_remote_value in per_worker_iterator._values.values():
       iterator_remote_value._type_spec = (
           input_lib.get_iterator_spec_from_dataset(
               self._coordinator.strategy, self._dataset_fn.structured_outputs))
diff --git a/tensorflow/python/training/server_lib.py b/tensorflow/python/training/server_lib.py
index da63fda73b2..c7f99d447b1 100644
--- a/tensorflow/python/training/server_lib.py
+++ b/tensorflow/python/training/server_lib.py
@@ -495,6 +495,47 @@ class ClusterSpec(object):
                           task_address)
         job_def.tasks[i] = task_address
 
+  def add_task(self, job_name, task_index, task_address):
+    """Add a task to the cluster.
+
+    Args:
+      job_name: The string name of a job, that may not be in the cluster.
+      task_index: A non-negative integer.
+      task_address: The address of added task.
+
+    Raises:
+      ValueError: If there is already a task with index `task_index`
+      in job `job_name`.
+    """
+    if job_name not in self._cluster_spec:
+      self._cluster_spec[job_name] = {}
+    elif task_index in self._cluster_spec[job_name]:
+      raise ValueError("There is Already a task with index %r in job %r" %
+                       (task_index, job_name))
+    self._cluster_spec[job_name][task_index] = task_address
+    self._make_cluster_def()
+
+  def remove_task(self, job_name, task_index):
+    """Remove a task from the cluster.
+
+    Args:
+      job_name: The string name of a job in this cluster.
+      task_index: A non-negative integer.
+
+    Raises:
+      ValueError: If there is no task with index `task_index`
+      in job `job_name`. Or if there is no job `job_name`.
+    """
+    if job_name in self._cluster_spec:
+      if task_index in self._cluster_spec[job_name]:
+        del self._cluster_spec[job_name][task_index]
+        self._make_cluster_def()
+      else:
+        raise ValueError("No task with index %r in job %r" %
+                  (task_index, job_name))
+    else:
+      raise ValueError("No such job in cluster: %r" % job_name)
+
 
 @tf_export("config.experimental.ClusterDeviceFilters")
 class ClusterDeviceFilters(object):
